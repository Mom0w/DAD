{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afandi Mohammad  \n",
    "## Hu Pauline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3PmMmh-3yoVf"
   },
   "source": [
    "# Convolutional Neural Networks (CNN) and Deep Architectures\n",
    "\n",
    "In these practicals, we will create a first convolutional neural network (CNN). This type of networks mostly rely on alternating layers of convolution and pooling. A convolution layer is a signal processing inspired layer that performs a bunch of convolutions on the layer input and yields filtered outputs known as feature maps. A pooling layer is a sort of subsampling layer. Most of the time, after several Conv/Pool layer pairs, a usual MLP is plugged to obtain class membership probabilities. When the number of layers is rather high (unclear how high this is), we obtain a deep network.\n",
    "\n",
    "We will use `TensorFlow` as programming framework and build a CNN to solve an image classification task.\n",
    "\n",
    "## Starting with TensorFlow\n",
    "\n",
    "`TensorFlow` is Google's library designed to create and train deep nets. It relies on symbolic programming and on the notion of computation graph. Here is an example of such a graph:\n",
    "\n",
    "<img src=\"graph.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "In this graph, we prepare `TensorFlow` to later compute the following function :\n",
    "\n",
    "$$f\\left( x,y\\right) = x^2 + 2y,$$\n",
    "\n",
    "for some $x$ and $y$. In this graph, the variables $x$ and $y$ are instances of the `tf.placeholder` class. Given these two objects, programming function $f$ is quite easy. We just need one line of code to explain `TensorFlow` how to compute $f\\left( x,y\\right)$ given $x$ and $y$:\n",
    "\n",
    "```python\n",
    "f = x*x + 2*y\n",
    "```\n",
    "\n",
    "This is done in a symbolic way meaning that $+$ here is not the usual Python + but `TensorFlow`'s addition operator. \n",
    "\n",
    "There are consequently two programming phases when using `TensorFlow`:\n",
    "* graph building and variable declaration,\n",
    "* actual computation performed as part of a `session`.\n",
    "\n",
    "Here is the full code to build the graph for function $f$ followed by a `session` to compute function values for some $(x,y)$ pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4668,
     "status": "ok",
     "timestamp": 1548020426316,
     "user": {
      "displayName": "Mohammad Afandi",
      "photoUrl": "",
      "userId": "02039134544224464075"
     },
     "user_tz": -60
    },
    "id": "3Fm33VzNyoVg",
    "outputId": "9dec5024-e110-4545-e5a1-4832dcf753c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4037,
     "status": "ok",
     "timestamp": 1548020426380,
     "user": {
      "displayName": "Mohammad Afandi",
      "photoUrl": "",
      "userId": "02039134544224464075"
     },
     "user_tz": -60
    },
    "id": "uq6LX6woyoVn",
    "outputId": "9e3a1b49-b3b6-4046-88ca-c7d537eaac22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "10.0\n",
      "17.0\n"
     ]
    }
   ],
   "source": [
    "# creates the variable container\n",
    "x = tf.placeholder(tf.float32,shape=[])\n",
    "y = tf.placeholder(tf.float32,shape=[])\n",
    "f = x*x + 2*y\n",
    "some_x = [1,2,3]\n",
    "some_y = [2,3,4]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(len(some_x)):\n",
    "        print(sess.run(f,feed_dict={x: some_x[i],y: some_y[i]})) # sets the variable's values and computes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PiApffUyoVu"
   },
   "source": [
    "The `tf.placeholder` class is meant to define variables that will be later replaced with data streams. Here, we specified that these data are `float32` and that they are scalars (`shape=[]`). In the `session`, we explain `TensorFlow` that the `x` and `y` will now be replaced by some specific values which we defined before as integer lists. \n",
    "\n",
    "### Q1\n",
    "* Define in a symbolic way the gradient of function $f$ (see `TensorFlow.gradients` documentation) and compute the gradient vectors for the same $(x,y)$ pairs as in the above code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3107,
     "status": "ok",
     "timestamp": 1548020426442,
     "user": {
      "displayName": "Mohammad Afandi",
      "photoUrl": "",
      "userId": "02039134544224464075"
     },
     "user_tz": -60
    },
    "id": "FQRkuaS6yoVu",
    "outputId": "7377f5ba-8b5f-4993-f7f6-2f06166c919f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 2.0]\n",
      "[4.0, 2.0]\n",
      "[6.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "# computes the gradient of the f function\n",
    "gradf = tf.gradients(f, [x,y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(len(some_x)):\n",
    "        print(sess.run(gradf,feed_dict={x: some_x[i],y: some_y[i]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ko-JD4aGyoVz"
   },
   "source": [
    "---\n",
    "\n",
    "## Baseline CNN\n",
    "\n",
    "We are now almost ready to start building a `TensorFlow` graph containing convolution or pooling layers instead of the basic operations involved in function $f$. Unlike function $f$, the classification function that we are trying to learn is parametric. Parameters can be explicitely defined as `TensorFlow` variables which are instances of the `tf.Variable` class. Since the backpropagation cannot work if parameters are not intialized, we will be using an initializer provided by `TensorFlow`. However, we will not explicitely define parameters but instead use `TensorFlow` macroscopic layerwise functions which encompass `TensorFlow` variables. This makes the code much simpler but less modular.\n",
    "\n",
    "But first things first ! We need a dataset to work with and to load in Jupyter. We will be working with the `MNIST` dataset. This dataset contains 28x28 grayscale images of handwritten figures. It has the advantage to be larger than traditional datasets utilized in practicals while it can still hold in memory and converge in a few minutes. Larger or more complicated datasets (more variability in images) would require too many computational efforts.\n",
    "\n",
    "Download the `MNIST` archive at [mnist.pkl.gz](http://deeplearning.net/data/mnist/mnist.pkl.gz). Load the dataset using the `load_mnist` function. You need to feed it with the path to the `MNIST` archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqqM1KwdyoV0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib.pyplot import imshow\n",
    "import gzip\n",
    "import _pickle as cPickle # for serialization (convert data to byte)\n",
    "\n",
    "def load_mnist(filename):\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    # the train, validation and test sets contain two arrays : the data with feature values and the target values\n",
    "    # train set 50000 values\n",
    "    # validation set : 10000 values\n",
    "    # test set : 10000 values\n",
    "    train_set, valid_set, test_set = cPickle.load(f, encoding='iso-8859-1')\n",
    "    f.close()\n",
    "    X = np.vstack((train_set[0],valid_set[0]))\n",
    "    y = np.hstack((train_set[1],valid_set[1]))\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "    return X,y,X_test,y_test\n",
    "\n",
    "# Load the mnist package and assign train, test sets and targets\n",
    "X,y,X_test,y_test = load_mnist('mnist.pkl.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urHp6favLkG0"
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)  # for numpy fuctions\n",
    "tf.set_random_seed(1234)  # for tensorflow functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNp9T7_kD87Q"
   },
   "outputs": [],
   "source": [
    "# shuffle the data first!\n",
    "index = np.arange(len(X))\n",
    "np.random.shuffle(index)\n",
    "X = X[index]\n",
    "y = y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYFvL-_JCw4k"
   },
   "source": [
    "The rows in X have been shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QY3xCNvyoV4"
   },
   "source": [
    "### Q2\n",
    "* Reshape `X` and `X_test` so that they have shape (60000,28,28,1) and (10000,28,28,1) respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kC0a7W2PyoV5"
   },
   "outputs": [],
   "source": [
    "# reshapping each row of X to get the shape of an image\n",
    "X = np.reshape(X,(60000,28,28,1))\n",
    "X_test = np.reshape(X_test,(10000,28,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhKHxFbIyoV9"
   },
   "source": [
    "We can now begin to build the CNN. At the start of the `TensorFlow` graph we need to define data containers, i.e. instances of `tf.placeholder`. We will use mini-batch gradient descent to optimize the NN parameters and we want the code to be modular so we cannot specify the first dimension of the tensor containing training examples. `TensorFlow` allows this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rcf_F2G5yoV-"
   },
   "outputs": [],
   "source": [
    "# creating the variable\n",
    "tf.reset_default_graph()\n",
    "Xtf = tf.placeholder(tf.float32,shape=[None, X.shape[1], X.shape[2],1]) # Xtf contains a modulable number of 28x28 arrays (the mnist images)\n",
    "ytf = tf.placeholder(tf.int64,shape=[None, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptepppR2yoWE"
   },
   "source": [
    "The next element of our NN is a 2D convolutional layer. We can use an instance of the `tf.layers.conv2d` class. We need to specify that a `tf.placeholder` instance is the input, the number of size of filters (kernels) that we want, how to deal with image borders (padding) and what activation we would like. This gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goRAdlx5yoWF"
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "      inputs=Xtf,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5], # size of the convolution window\n",
    "      padding=\"same\",  # how to handle borders ('valid': ignores sides values if filter size and strides aren't fit to go theough the entire image,\n",
    "                       #                        'same': adds dummy values to convolve through the entire image)\n",
    "      activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YVbItVVryoWK"
   },
   "source": [
    "Next, we need a pooling layer to give up some entries of the filtered images provided by the convolutional layer. We will use the usual max pool function in 2x2 neighborhoods which drops 3 entries out of 4 in each such neighborhood. The remaining entry is the one achieving maximal value in the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YpZNR-ayoWL"
   },
   "outputs": [],
   "source": [
    "# Pooling Layer #1\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sxd-gDbYyoWO"
   },
   "source": [
    "### Q3\n",
    "* What is the role of parameter `strides` ? \n",
    "* What is the size of each feature map after max pooling ?\n",
    "* How many feature maps are there ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CS3GC2dCDtjK"
   },
   "source": [
    "**Aswer Q3:**  \n",
    "* Stride controls how the filter convolves around the input volume. The amount by which the filter shifts is the stride. Stride is normally set in a way so that the output volume is an integer and not a fraction. It can be interpreted as the step of the filter.\n",
    "* The size of each feature map after max pooling is 14x14.\n",
    "Indeed the feature map contains the values of each convolution the filter computes while going though the entire image. If the image size was 28x28, the filter size 5x5, the stride 1 and the padding set to 'valid', the filter would go through $(28-5+1) \\times (28-5+1)$ different locations. The feature map size would be 24x24.    \n",
    "Here the padding is set to 'same'. This means he output has the same size as the intput : 28x28.  \n",
    "The basic pooling method consists in applying a filter with a stride of same length on the feature map. For each stride, we keep the highest value. The new feature map has its size reduced.\n",
    "Indeed applying a pooling layer on the feature map helps reducing the amount of data in the feature map while preserving its global structure. It also helps with overfitting since it only keeps important values and removes less important ones. In our case we have a pool filter of size 2x2, a feature map of size 28x28 and a stride of 2. The final feature map is of size $(28*28)/ ((4/2)*(4/2))$: 14x14.\n",
    "* There are 64 different feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGSRq8Di-SZi"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZ2R9zK5-OP5"
   },
   "source": [
    "We will now plug a 1 layer MLP (multilayer perceptron) on top of this. Fully connected layers (or dense layers) do not accept numpy ndarrays. We can use tf.reshape to flatten feature maps. In the following code the scalar dflat = nbr of feature maps x feature map width x feature map height. Set this variable with the correct value and execute the following code line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDoJb714yoWP"
   },
   "outputs": [],
   "source": [
    "# flatten pooled array for MLP\n",
    "dflat = 64*14*14\n",
    "pool1_flat = tf.reshape(pool1, [-1, dflat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlVgsSOlyoWS"
   },
   "source": [
    "Now the one layer MLP is given by an instance of `tf.layers.dense` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2ZJInityoWU"
   },
   "outputs": [],
   "source": [
    "# add one hidden layer with 1024 neurons\n",
    "mlp = tf.layers.dense(inputs=pool1_flat, units=1024, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nghQ8amyoWX"
   },
   "source": [
    "Finally, we need a softmax regression function to obtain class membership probabilities. This can be done in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsRVA5oDyoWY"
   },
   "outputs": [],
   "source": [
    "# apply classification function : logit (logistic regression generalization for K > 2 classes)\n",
    "logits = tf.layers.dense(inputs=mlp, units=10) \n",
    "smax = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIbtHFPOyoWb"
   },
   "source": [
    "### Q4\n",
    "* How many parameters is there in our NN ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJ2I8uCn7A7Z"
   },
   "source": [
    "---\n",
    "**NN parameters :** \n",
    "- convolution : size of convolution window, stride, how to handle borders, number of filters\n",
    "- pooling : size of pooling window, stride and how to handle borders\n",
    "- activation function \n",
    "- classification function\n",
    "- size of the mini-batch\n",
    "- optimizer learning rate\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMg67j094Fjs"
   },
   "source": [
    "Actually, the utimate element of the `TensorFlow` graph is the loss. In softmax (or logistic) regression, the corresponding loss is the cross entropy. A numerically stable version of this loss is given by `tf.losses.sparse_softmax_cross_entropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxwxrXMDyoWc"
   },
   "outputs": [],
   "source": [
    "# computes the loss function for training (on logits because losses.sparse_softmax_cross_entropy already applies smax)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=ytf, logits=logits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2RY5IIByoWg"
   },
   "source": [
    "Now that we have completed the graph, we can move to the next phase: actually training the NN. Prior to entering the training loop, we need to instantiate an optimizer from `TensorFlow` and ask him to minimize the loss. We will use the ADAM optimizer which is robust and easy to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-A0EpyixyoWi"
   },
   "outputs": [],
   "source": [
    "# set the optimizer used for the backpropagation\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss) \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLx4gVoO_0E6"
   },
   "source": [
    "Adam optimization algorithm is different from stochastical gradient descent in that it changes the learning rate (speed of descent) at each step of the descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuqOHf3TyoWn"
   },
   "source": [
    "An initializer has also been instantiated. Indeed parameters are hidden in instances of our NN layers but they need initialization anyway.\n",
    "\n",
    "We can now begin to iterate on training examples. We will select mini-batches of training examples. Once each training example has been processed as part of the backpropagation algorithm, a training epoch is completed. It is quite common to iterate on epochs, and in each epoch on mini-batches. \n",
    "\n",
    "The following code lines are in line with this habit. There are only two lines to complete to make it work. They correpond to the selection of training examples in each mini-batch. \n",
    "\n",
    "### Q5\n",
    "* Fulfill correctly the missing parts of these instructions.\n",
    "**Warning** : the execution of the loop takes around 5mn. The gradient descent should achieve approximately a 0.13 loss or below (remember this is stochastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ynh6SWUEHlyI"
   },
   "outputs": [],
   "source": [
    "# Accuracy metrics\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), ytf) # get the predicted target with maximum weigth with argmax and compare it to the real target value\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   # casting the correct_prediction to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 262237,
     "status": "ok",
     "timestamp": 1548023041611,
     "user": {
      "displayName": "Mohammad Afandi",
      "photoUrl": "",
      "userId": "02039134544224464075"
     },
     "user_tz": -60
    },
    "id": "KjDM_oEFBdyH",
    "outputId": "de9f165f-ce54-45b3-9972-7629661fbbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.12863576 , Accuracy: 0.9866667\n",
      "1 Train loss: 0.08623568 , Accuracy: 0.99333334\n",
      "\n",
      "Test Accuracy 0.9794\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "batch_size = 150\n",
    "n = len(X)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # training cycles\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = n // batch_size\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()  # fluhing the buffer \n",
    "            X_batch = X[iteration*batch_size : (iteration+1)*batch_size]\n",
    "            y_batch = y[iteration*batch_size : (iteration+1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={Xtf: X_batch,ytf: y_batch})  # running the optimizer\n",
    "                        \n",
    "        loss_train = loss.eval(feed_dict={Xtf: X_batch,ytf: y_batch})\n",
    "        train_accuracy = sess.run(accuracy,feed_dict={Xtf: X_batch,ytf: y_batch}) # compute training accuracy\n",
    "        print(\"\\r{}\".format(epoch), \"Train loss:\", loss_train , \", Accuracy:\", train_accuracy)\n",
    "        \n",
    "    test_accuracy = sess.run(accuracy,feed_dict={Xtf: X_test,ytf: y_test})\n",
    "    print(\"\\nTest Accuracy\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kC2gw2tqyoWy"
   },
   "source": [
    "### Q6\n",
    "* Document yourselves on means to compute the classifier accuracy using `TensorFlow` functions. \n",
    "* Add the corresponding functions to the graph.\n",
    "* Modify the loop to evaluate the classifier accuracy on the test set. \n",
    "* Comment on the quality of the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VP9tVggRFHaL"
   },
   "source": [
    "---\n",
    "The train loss during the second execution of the training is lower because the NN has already been trained on the train set one time. Training NN networks two times rises the train accuracy too. \n",
    "\n",
    "The test accuracy rate is very good, meaning the built model hasn't been overfitted. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdcsdbLb8UkP"
   },
   "source": [
    "### Q7\n",
    "* Add a second pair of conv/pool layers. A smaller amount of filters and smaller filters are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SoL6t3Ca8bBP"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "Xtf = tf.placeholder(tf.float32,shape=[None, X.shape[1], X.shape[2],1])\n",
    "ytf = tf.placeholder(tf.int64,shape=[None, ])\n",
    "\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "      inputs=Xtf,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5], # size of the convolution window\n",
    "      padding=\"same\",  # how to handle borders ('valid': ignores sides values if filter size and strides aren't fit to go theough the entire image,\n",
    "                       #                        'same': adds dummy values to convolve through the entire image)\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "# Pooling Layer #1\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1, # added on top of first layer\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3], # size of the convolution window (smaller window)\n",
    "      padding=\"same\",  # how to handle borders ('valid': ignores sides values if filter size and strides aren't fit to go theough the entire image,\n",
    "                       #                        'same': adds dummy values to convolve through the entire image)\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "# conv2 has same height and width as pool1 (14x14)\n",
    "\n",
    "# Pooling Layer #2\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Flatten pooled array for MLP: pool2 applies a 50% reduction of height and width from conv2\n",
    "dflat2 = 32*7*7\n",
    "pool2_flat = tf.reshape(pool2, [-1, dflat2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oko5KZme8cFa"
   },
   "source": [
    "### Q8\n",
    "* Add a second fully connected layer. A smaller number of units is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wonMUUOq8eyX"
   },
   "outputs": [],
   "source": [
    "# Add one hidden layer with 1024 neurons\n",
    "mlp1 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "# Add another hidden layer on top of the first one with 128 neurons\n",
    "mlp2 = tf.layers.dense(inputs=mlp1, units=128, activation=tf.nn.relu)\n",
    "\n",
    "# Apply classification function : logit (logistic regression generalization for K > 2 classes)\n",
    "logits = tf.layers.dense(inputs=mlp2, units=10) \n",
    "smax = tf.nn.softmax(logits)\n",
    "\n",
    "# Computes the loss function for training\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=ytf, logits=logits)\n",
    "\n",
    "# Set the optimizer used for the backpropagation\n",
    "training_op = optimizer.minimize(loss) \n",
    "\n",
    "# Accuracy metrics\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), ytf)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Variables initializer\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 262237,
     "status": "ok",
     "timestamp": 1548023041611,
     "user": {
      "displayName": "Mohammad Afandi",
      "photoUrl": "",
      "userId": "02039134544224464075"
     },
     "user_tz": -60
    },
    "id": "KjDM_oEFBdyH",
    "outputId": "de9f165f-ce54-45b3-9972-7629661fbbe6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.112862 , Accuracy: 0.99333334\n",
      "1 Train loss: 0.0792609 , Accuracy: 0.99333334\n",
      "\n",
      "Test Accuracy 0.9859\n"
     ]
    }
   ],
   "source": [
    "# Training graph with 2 layers\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 150\n",
    "n = len(X)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # training cycles\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = n // batch_size\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()  # fluhing the buffer \n",
    "            X_batch = X[iteration*batch_size : (iteration+1)*batch_size]\n",
    "            y_batch = y[iteration*batch_size : (iteration+1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={Xtf: X_batch,ytf: y_batch})  # running the optimizer\n",
    "                        \n",
    "        loss_train = loss.eval(feed_dict={Xtf: X_batch,ytf: y_batch})\n",
    "        train_accuracy = sess.run(accuracy,feed_dict={Xtf: X_batch,ytf: y_batch}) # compute training accuracy\n",
    "        print(\"\\r{}\".format(epoch), \"Train loss:\", loss_train , \", Accuracy:\", train_accuracy)\n",
    "        \n",
    "    test_accuracy = sess.run(accuracy,feed_dict={Xtf: X_test,ytf: y_test})\n",
    "    print(\"\\nTest Accuracy\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HmvWompGFZ2"
   },
   "source": [
    "By adding more layers, we've been able to get better loss and accuracy results.  However, it also causes the training phase to take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItzoS5yT8fsa"
   },
   "source": [
    "---\n",
    "### Q9\n",
    "From the above NN architecture, try to tweak the other hyperparameters. These latter include:\n",
    "* the learning rate,\n",
    "* the mini batch size,\n",
    "* the regularization strength (if regularization is activated),\n",
    "* number of filters,\n",
    "* filter sizes,\n",
    "* activiation functions,\n",
    "* strides,\n",
    "* padding types,\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5B36nCl8i7U"
   },
   "outputs": [],
   "source": [
    "def nn_2layers(X, y, X_test, y_test, learning_rate, batch_size):\n",
    "  \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=Xtf,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5], \n",
    "        padding=\"same\",  \n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1, \n",
    "        filters=32,\n",
    "        kernel_size=[3, 3], \n",
    "        padding=\"same\",  \n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    dflat2 = 32*7*7\n",
    "    pool_flat = tf.reshape(pool2, [-1, dflat2])\n",
    "\n",
    "    mlp1 = tf.layers.dense(inputs=pool_flat, units=1024, activation=tf.nn.relu)\n",
    "    mlp2 = tf.layers.dense(inputs=mlp1, units=128, activation=tf.nn.relu)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=mlp2, units=10) \n",
    "    smax = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=ytf, logits=logits) \n",
    "\n",
    "    training_op = tf.train.AdamOptimizer(learning_rate).minimize(loss) \n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), ytf)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    n_epochs = 2\n",
    "    n = len(X)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "\n",
    "        # training cycles\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = n // batch_size\n",
    "\n",
    "            # Loop over all batches\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()  # fluhing the buffer \n",
    "                X_batch = X[iteration*batch_size : (iteration+1)*batch_size]\n",
    "                y_batch = y[iteration*batch_size : (iteration+1)*batch_size]\n",
    "                sess.run(training_op, feed_dict={Xtf: X_batch,ytf: y_batch})  # running the optimizer\n",
    "\n",
    "            loss_train = loss.eval(feed_dict={Xtf: X_batch, ytf: y_batch})\n",
    "            train_accuracy = sess.run(accuracy,feed_dict={Xtf: X_batch,ytf: y_batch}) # compute training accuracy\n",
    "            print(\"\\r{}\".format(epoch), \"Train loss:\", loss_train , \", Train accuracy:\", train_accuracy)\n",
    "\n",
    "        test_accuracy = sess.run(accuracy,feed_dict={Xtf: X_test, ytf: y_test})\n",
    "        print(\"\\nTest Accuracy\", test_accuracy)\n",
    "\n",
    "    return test_accuracy\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 262237,
     "status": "ok",
     "timestamp": 1548023041611,
     "user": {
      "displayName": "Mohammad Afandi",
      "photoUrl": "",
      "userId": "02039134544224464075"
     },
     "user_tz": -60
    },
    "id": "KjDM_oEFBdyH",
    "outputId": "de9f165f-ce54-45b3-9972-7629661fbbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.11862265 , Train accuracy: 0.9866667\n",
      "1 Train loss: 0.112266056 , Train accuracy: 0.99333334\n",
      "\n",
      "Test Accuracy 0.9742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9742"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "Xtf = tf.placeholder(tf.float32,shape=[None, X.shape[1], X.shape[2],1])\n",
    "ytf = tf.placeholder(tf.int64,shape=[None, ])\n",
    "nn_2layers(X, y, X_test, y_test, learning_rate = 0.01, batch_size = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7IsrRD8YEqz"
   },
   "source": [
    "Let's assess the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwOyDNF-cZDq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.16074069 , Train accuracy: 0.9866667\n",
      "1 Train loss: 0.14980233 , Train accuracy: 0.9866667\n",
      "\n",
      "Test Accuracy 0.9683\n",
      "0 Train loss: 0.11688807 , Train accuracy: 0.99333334\n",
      "1 Train loss: 0.111937754 , Train accuracy: 0.99333334\n",
      "\n",
      "Test Accuracy 0.9855\n",
      "0 Train loss: 0.3298403 , Train accuracy: 0.9266667\n",
      "1 Train loss: 0.112037696 , Train accuracy: 0.99333334\n",
      "\n",
      "Test Accuracy 0.9658\n",
      "0 Train loss: 2.3061216 , Train accuracy: 0.10666667\n",
      "1 Train loss: 2.3062003 , Train accuracy: 0.10666667\n",
      "\n",
      "Test Accuracy 0.101\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "learning_rate_acc = np.zeros(len(learning_rates))\n",
    "\n",
    "for i, rate in enumerate(learning_rates):\n",
    "    tf.reset_default_graph()\n",
    "    Xtf = tf.placeholder(tf.float32,shape=[None, X.shape[1], X.shape[2],1])\n",
    "    ytf = tf.placeholder(tf.int64,shape=[None, ])\n",
    "    learning_rate_acc[i] = nn_2layers(X, y, X_test, y_test, learning_rate = rate, batch_size = 150)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rates,learning_rate_acc)\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Influence of learning rate on test accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the learning rate should be kept between 1e-4 and 1e-3.  \n",
    "The last value is rather strange, we wouldn't expect such a drop in performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to tweak the size of the batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 6.818976 , Train accuracy: 0.098733336\n",
      "1 Train loss: 2.3034468 , Train accuracy: 0.17096667\n",
      "\n",
      "Test Accuracy 0.1736\n",
      "0 Train loss: 2.2385414 , Train accuracy: 0.1827\n",
      "1 Train loss: 0.93954235 , Train accuracy: 0.6911\n",
      "\n",
      "Test Accuracy 0.6754\n",
      "0 Train loss: 2.3030875 , Train accuracy: 0.107\n",
      "1 Train loss: 2.302242 , Train accuracy: 0.107\n",
      "\n",
      "Test Accuracy 0.1028\n",
      "0 Train loss: 0.12807916 , Train accuracy: 0.97\n",
      "1 Train loss: 0.15642028 , Train accuracy: 0.99\n",
      "\n",
      "Test Accuracy 0.9554\n"
     ]
    }
   ],
   "source": [
    "l_batch_sizes = [30000, 10000, 1000, 100]\n",
    "l_batch_sizes_acc = np.zeros(len(learning_rates))\n",
    "\n",
    "for i,size in enumerate(l_batch_sizes):\n",
    "    tf.reset_default_graph()\n",
    "    Xtf = tf.placeholder(tf.float32,shape=[None, X.shape[1], X.shape[2],1])\n",
    "    ytf = tf.placeholder(tf.int64,shape=[None, ])\n",
    "    l_batch_sizes_acc[i] = nn_2layers(X, y, X_test, y_test, learning_rate = 0.01, batch_size = size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l_batch_sizes,l_batch_sizes_acc)\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Influence of batch size on test accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the lower the better, even though that means more optimizing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "These results should be taken with a grain of salt, as we've only been able to run a few simulations because of how long each one takes.  \n",
    "Furthermore, we've also noticed that our simulations would randomly return inconsistant values after a given time. This may have to do with the tensorflow library or our personal computers limitations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kn6AEogZ8jV6"
   },
   "source": [
    "### Q10\n",
    "* Compare the obtained accuracy to that of a regularized Softmax regression from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzIDoMl4qfTy"
   },
   "source": [
    "---\n",
    "We should first reshape `X` and `X_test` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FkHJnnZpoBn"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_smax = np.reshape(X,(60000,784))\n",
    "X_smax_test = np.reshape(X_test,(10000,784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply softmax regression and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttbDHmHzol-k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=0, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial', penalty = 'l2')\n",
    "clf.fit(X_smax, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_LtBdB-oqNZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349833333333334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_smax, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's compute the accuracy of the predicions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f64JdVuLpSIt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9261"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_smax_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7THkBciKpWSA"
   },
   "source": [
    "The neural-network gives better results than the scikit-learn softmax regression.\n",
    "This may be because of the convolutional and pooling layers used in the neural network. These steps allows the model to actually take into consideration the global structure of the image. On the other hand, the softmax regression considers each feature (the pixels of the picture) with no regards to its neighbours.  \n",
    "However, it is significantly longer to train the neural network."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP6_Afandi_Hu.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
